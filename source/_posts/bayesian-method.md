---
title: 贝叶斯公式、先验概率、最大似然估计、最大后验概率估计
tags: [bayes]
categories: algorithm
date: 2018-7-30
---

该文是对以下博客的摘抄和总结：
- [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)
- [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)

# 贝叶斯公式的背景
"所谓的贝叶斯方法源于他生前为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后才由他的一位朋友发表出来的。在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。这个问题，就是所谓的逆概问题。"
这个时候，我们就需要提供一个猜测（hypothesis，更为严格的说法是“假设”，这里用“猜测”更通俗易懂一点），所谓猜测，当然就是不确定的（很可能有好多种乃至无数种猜测都能满足目前的观测），但也绝对不是两眼一抹黑瞎蒙——具体地说，我们需要做两件事情：1. 算出各种不同猜测的可能性大小。2. 算出最靠谱的猜测是什么。第一个就是计算特定猜测的后验概率，对于连续的猜测空间则是计算猜测的概率密度函数。第二个则是所谓的模型比较，模型比较如果不考虑先验概率的话就是最大似然方法。
# 例子：女裤style，导出“贝叶斯公式”
问题：一所学校里面有 $60\%$ 的男生和$40\%$ 的女生。男生总是穿长裤，女生则一半穿长裤一半穿裙子。有了这些信息之后我们可以容易地计算“随机选取一个学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。然而，假设你走在校园中，迎面走来一个穿长裤的学生（很不幸的是你高度近似，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是男生的概率是多大吗？
分析：一些认知科学的研究表明，我们对形式化的贝叶斯问题不擅长，但对于以频率形式呈现的等价问题却很擅长。在这里，我们不妨把问题重新叙述成：你在校园里面随机游走，遇到了 $N$ 个穿长裤的人（仍然假设你无法直接观察到他们的性别），问这$N$个人里面有多少个女生多少个男生。
解答：假设学校里面人的总数是 $U$ 个。$60\%$ 的男生都穿长裤，于是我们得到了 `$U*P(Boy)*P(Pants|Boy)$` 个穿长裤的（男生）（其中 $P(Boy)$ 是男生的概率 $= 60\%$，这里可以简单的理解为男生的比例；$P(Pants|Boy)$ 是条件概率，即在 Boy 这个条件下穿长裤的概率是多大，这里是 $100\%$ ，因为所有男生都穿长裤）。$40\%$ 的女生里面又有一半（$50\%$）是穿长裤的，于是我们又得到了 `$U* P(Girl) * P(Pants|Girl)$` 个穿长裤的（女生）。加起来一共是 `$U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)$` 个穿长裤的，其中有 `$U * P(Girl) * P(Pants|Girl)$` 个女生。两者一比就是你要求的答案。
转为贝叶斯问题：我们要求的是 $P(Girl|Pants)$ （穿长裤的人里面有多少女生），我们计算的结果是 `$\frac{U * P(Girl) * P(Pants|Girl)}{U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)}$`。容易发现这里校园内人的总数是无关的，可以消去。于是得到:
$$
P(Girl|Pants)=\frac{P(Girl) * P(Pants|Girl)}{P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)}
$$
注意，如果把上式收缩起来，分母其实就是 $P(Pants)$ ，分子其实就是 $P(Pants, Girl)$ 。而这个比例$P(Girl|Pants)$很自然地就读作：在穿长裤的人（ $P(Pants)$ ）里面有多少（穿长裤）的女孩（ $P(Pants, Girl)$ ）。

上式中的 Pants 和 Boy/Girl 可以指代一切东西，所以其一般形式就是：
$$
P(B|A) = \frac{P(A|B) * P(B)} {P(A|B) * P(B) + P(A|\sim B) * P(\sim B)}
$$
收缩起来就是：
$$
P(B|A) = \frac{P(AB)} {P(A)} = \frac{P(A|B) * P(B)}{P(A)}
$$

# 例子：拼写检查，导出“先验概率、最大似然估计和最大后验概率估计”
问题：我们看到用户输入了一个不在字典中的单词，我们需要去猜测：“这个家伙到底真正想输入的单词是什么呢？”。用刚才我们形式化的语言来叙述就是，我们需要求：$ P(我们猜测他想输入的单词 | 他实际输入的单词)$ 这个概率。并找出那个使得这个概率最大的猜测单词。显然，我们的猜测未必是唯一的，就像前面举的那个自然语言的歧义性的例子一样；这里，比如用户输入： thew ，那么他到底是想输入 the ，还是想输入 thaw ？到底哪个猜测可能性更大呢？幸运的是我们可以用贝叶斯公式来直接出它们各自的概率，我们不妨将我们的多个猜测记为 $h1, h2 .. (h=hypothesis)$，它们都属于一个有限且离散的猜测空间 $H$（单词总共就那么多而已），将用户实际输入的单词记为 $D, D 代表 Data ，即观测数据$，于是
$P(我们的猜测1 | 他实际输入的单词)$ 可以抽象地记为：$P(h1 | D)$，类似地，对于我们的猜测2，则是 $P(h2 | D)$。不妨统一记为：$P(h | D)$。那么运用一次贝叶斯公式，有：
$$
P(h|D) = \frac{P(h) * P(D|h)}{P(D)}
$$
对于不同的具体猜测$h1, h2, h3..$，$P(D)$都是一样的，所以在比较$P(h1 | D)$和$P(h2 | D)$时，可以忽略这个常数。即：
$$
P(h|D) \propto P(h) * P(D|h)
$$
这个式子的抽象含义是：对于给定观测数据，一个猜测是好是坏，取决于“这个猜测本身独立的可能性大小（先验概率，Prior ）”和“这个猜测生成我们观测到的数据的可能性大小”（似然，Likelihood ）的乘积。具体到我们的那个 thew 例子上，含义就是，用户实际是想输入 the 的可能性大小取决于 the 本身在词汇表中被使用的可能性（频繁程度）大小（先验概率）和 想打 the 却打成 thew 的可能性大小（似然）的乘积。下面的事情就很简单了，对于我们猜测为可能的每个单词计算一下 $P(h) * P(D | h)$ 这个值，然后取最大的，得到的就是最靠谱的猜测。
注意：贝叶斯方法计算的是两个概率的乘积，一个是先验概率，一个是最大似然估计。不能仅取其中一个，因为会有失偏颇。最大似然估计是计算了在$h$出现的情形下$D$出现的概率，但忽略了$h$本身出现的概率。如果$h$本身就很难出现，那么即使$P(D|h)$很大，那计算出的$P(h|D)$也会很小。比如如果用户输入的是tlp，那么到底他输入的是top还是tpp。当输入tpp时，有很大可能输成tlp，但tpp本身出现概率很小，所以更有可能是用户想输的是top。
奥卡姆剃刀就是说 $P(h)$ 较大的模型有较大的优势，而最大似然则是说最符合观测数据的（即 $P(D | h)$ 最大的）最有优势。整个模型比较就是这两方力量的拉锯。更一般的，大多数情形下我们的先验概率往往是均匀分布，此时就主要是似然估计在起作用。但即使只有似然估计起作用的时候，它也是倾向于更简单的模型，即此时是贝叶斯奥卡姆剃刀。

所以，贝叶斯方法计算的是最大后验概率估计，它包括两个部分：先验概率和最大似然估计。具体地，似然估计是求的参数，是根据已有的数据，推知模型中的参数，以使得该数据出现的可能性最大。
背景知识：
概率（probabilty）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。
概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。
对于函数：
$$
P(x|\theta)
$$
其中，$x$表示数据，$\theta$表示模型的参数。
如果$\theta$已知，$x$是变量，那么这个函数叫做概率函数（probability function），描述对于不同的样本点$x$，其出现概率是多少。如果$x$已知，$\theta$是变量，那么这个函数叫做似然函数（likelihood function），描述对于不同的模型参数，出现$x$这个样本点的概率是多少。

# 例子：垃圾邮件过滤器，导出“朴素贝叶斯”
问题：给定一封邮件，判定它是否属于垃圾邮件。按照先例，我们还是用 $D$ 来表示这封邮件，注意 $D$ 由 $N$ 个单词组成。我们用 $h$ 来表示垃圾邮件。问题可以形式化地描述为求：
$$
P(h|D) = P(h) * P(D|h) / P(D)
$$
其中 $P(h)$ 这个先验概率是很容易求出来的，只需要计算一个邮件库里面垃圾邮件占所有邮件的比例就行了。然而 $P(D|h)$ 却不容易求，因为 $D$ 里面含有 $N$ 个单词 $d1, d2, d3, ..$ ，所以$P(D|h) = P(d1,d2,..,dn|h)$ 。我们又一次遇到了数据稀疏性，为什么这么说呢？$P(d1,d2,..,dn|h)$ 就是说在垃圾邮件当中出现跟我们目前这封邮件一模一样的一封邮件的概率是多大！开玩笑，每封邮件都是不同的，世界上有无穷多封邮件。瞧，这就是数据稀疏性，因为可以肯定地说，你收集的训练数据库不管里面含了多少封邮件，也不可能找出一封跟目前这封一模一样的。结果呢？我们又该如何来计算它呢？我们将 $P(d1,d2,..,dn|h)$  扩展为： `$P(d1|h) * P(d2|d1, h) * P(d3|d2,h) * ..$`。这个式子就是假设一个词出现的概率只依赖于它前面的一个词。这里我们会使用一个更激进的假设，我们假设 $di$ 与 $di-1$ 是完全条件无关的，于是式子就简化为 `$P(d1|h) * P(d2|h) * P(d3|h) * ..$` 。这个就是所谓的条件独立假设，也正是朴素贝叶斯方法的朴素之处。而计算 `$P(d1|h) * P(d2|h) * P(d3|h) * ..$` 就太简单了，只要统计 $di$ 这个单词在垃圾邮件中出现的频率即可。

